{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1412434d",
   "metadata": {},
   "source": [
    "# Train VAE to generate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd15d4",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/data/human_virus_600k_seq_label_20aa.csv\")\n",
    "seq_20aa = df['sequence'].to_list()\n",
    "label_seq = df['label'].to_list()\n",
    "label_20aa = [1 if v == 'human' else 0 for v in label_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d38bfa",
   "metadata": {},
   "source": [
    "### For accelerate training, we compute the embeddings of the whole dataset first.\n",
    "\n",
    "To speed up training, we precompute embedding to avoid memory overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "seq = seq_20aa\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")  # ESM model path, you can down load from https://huggingface.co/facebook/esm2_t33_650M_UR50D\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "ESMmodel = EsmModel.from_pretrained(\"./post_train_esm/checkpoint-14980\").to(device) # using ESM or LoRA-ESM to get embedding feature\n",
    "ESMmodel = torch.nn.DataParallel(ESMmodel)  \n",
    "\n",
    "batch_size = 4000  # Adjust this value according to your memory situation\n",
    "\n",
    "batches_tcra = [seq[i:i+batch_size] for i in range(0, len(seq), batch_size)]\n",
    "\n",
    "all_tcra_last_hidden_states = []\n",
    "\n",
    "for batch_tcra in tqdm(batches_tcra):  \n",
    "    batch_tcra_inputs = tokenizer(batch_tcra, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20+2).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_tcra_outputs = ESMmodel(**batch_tcra_inputs)\n",
    "\n",
    "    last_hidden_state = batch_tcra_outputs.last_hidden_state.cpu()\n",
    "    all_tcra_last_hidden_states.append(last_hidden_state)\n",
    "    \n",
    "\n",
    "total_seq_last_hidden_states = torch.cat(all_tcra_last_hidden_states, dim=0)\n",
    "\n",
    "torch.save(total_seq_last_hidden_states[:200000,:-1,:],\"esm_human_virus_0to200000\")\n",
    "torch.save(total_seq_last_hidden_states[200000:400000,:-1,:],\"esm_human_virus_200000to400000\")\n",
    "torch.save(total_seq_last_hidden_states[400000:,:-1,:],\"esm_human_virus_400000to610760\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de95912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# This will take up ~200G of memory footprint\n",
    "esm_data_tmp1 = torch.load(\"esm_human_virus_0to200000\")\n",
    "esm_data_tmp2 = torch.load(\"esm_human_virus_0to200000\")\n",
    "esm_data_tmp3 = torch.load(\"esm_human_virus_0to200000\")\n",
    "\n",
    "esm_data=torch.cat((esm_data_tmp1,esm_data_tmp2,esm_data_tmp3),dim=0)\n",
    "esm_data.shape\n",
    "\n",
    "del esm_data_tmp1\n",
    "del esm_data_tmp2\n",
    "del esm_data_tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human Virus dataset & dataloader\n",
    "import encoding_matrix\n",
    "from data_util import seq_encoding_with_matrix, data_padding\n",
    "\n",
    "train_peptide_human_virus = [seq_encoding_with_matrix(seq,matrix=encoding_matrix.NUMBER_MATRIX) for seq in seq_20aa]\n",
    "\n",
    "padding_seq = 'A'*20 \n",
    "padding_seq_NUMBER = [seq_encoding_with_matrix(padding_seq,matrix=encoding_matrix.NUMBER_MATRIX)]\n",
    "\n",
    "padding_type = 'end'\n",
    "all_NUMBER_encoding = data_padding(padding_seq_NUMBER+train_peptide_human_virus,padding_type)[1:,:,:]\n",
    "\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "train_pep, X_temp, train_labels, label_temp, train_esm, esm_temp = train_test_split(all_NUMBER_encoding, label_20aa, esm_data, test_size=0.3, random_state=42)  \n",
    "valid_pep, test_pep, valid_labels, test_labels, valid_esm, test_esm = train_test_split(X_temp, label_temp, esm_temp, test_size=0.5, random_state=42)  \n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tcra, esm, label):\n",
    "        self.tcra_inputs = tcra\n",
    "        self.label = torch.tensor(label,dtype=torch.long)\n",
    "        self.esm = esm\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tcra_inputs[index], self.esm[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tcra_inputs)\n",
    "\n",
    "\n",
    "train_dataset = TrainDataset(train_esm, train_pep, train_labels)\n",
    "valid_dataset = TrainDataset(valid_esm, valid_pep, valid_labels)\n",
    "test_dataset = TrainDataset(test_esm, test_pep, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4bed5",
   "metadata": {},
   "source": [
    "### VAE model\n",
    "\n",
    "actually we only train the decoder and classifier, the encoder is fixed for the computation resource constrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d112818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def apply_rope(x, seq_len, head_dim):\n",
    "    theta = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=x.device).float() / head_dim))\n",
    "    seq_positions = torch.arange(0, seq_len, device=x.device).float().unsqueeze(1)\n",
    "    cos = torch.cos(seq_positions * theta).unsqueeze(0).unsqueeze(0)\n",
    "    sin = torch.sin(seq_positions * theta).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "    x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "    return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output linear projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Get batch size and sequence lengths\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_proj(query)  # (batch_size, seq_len, embed_dim)\n",
    "        K = self.k_proj(key)    # (batch_size, seq_len, embed_dim)\n",
    "        V = self.v_proj(value)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Split into multiple heads and reshape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Apply RoPE to queries and keys\n",
    "        Q = apply_rope(Q, seq_len, self.head_dim)\n",
    "        K = apply_rope(K, seq_len, self.head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.out_proj(attention_output)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Decoder_esm_to_seq_with_Contrastive(nn.Module):\n",
    "    def __init__(self, decoder_hidden_size, decoder_num_heads):\n",
    "        super(Decoder_esm_to_seq_with_Contrastive, self).__init__()\n",
    "\n",
    "        # parameter\n",
    "        self.decoder_hidden_size = decoder_hidden_size # 1280\n",
    "        self.decoder_num_heads = decoder_num_heads\n",
    "\n",
    "        # network\n",
    "        self.re_pos_net_linear = nn.Linear(self.decoder_hidden_size,self.decoder_hidden_size)\n",
    "        self.re_pos_net_attn = MultiHeadAttention(self.decoder_hidden_size , 4)\n",
    "\n",
    "        self.hidden_to_aa = nn.Linear(self.decoder_hidden_size, 21)\n",
    "\n",
    "    \n",
    "    def forward(self, esm_x, label):\n",
    "        esm_x = self.re_pos_net_linear(esm_x)\n",
    "        # esm_x,_ = self.re_pos_net_attn(esm_x, esm_x, esm_x)\n",
    "        decoder_ids = self.hidden_to_aa(esm_x) # attn_out:[batch_size, max_seq_length, decoder_hidden_size] -> decoder_ids:[batch_size, max_seq_length, 21]\n",
    "        decoder_ids = nn.functional.elu(decoder_ids) \n",
    "        decoder_ids = decoder_ids.permute(0,2,1) # decoder_ids:[batch_size, max_seq_length, 21] -> decoder_ids:[batch_size, 21, max_seq_length] 直接permute，在AA之间加入attention会大幅度降低最终还原的准确性\n",
    "        return decoder_ids, esm_x, label\n",
    "\n",
    "\n",
    "def loss_function2(recon_x, x, re_pos_esm, labels, margin=200):\n",
    "    BCE = nn.functional.cross_entropy(recon_x, x, label_smoothing=0.2)\n",
    "    re_pos_esm = re_pos_esm.reshape(re_pos_esm.shape[0], -1)\n",
    "    categories = labels.unsqueeze(1)  \n",
    "    pairwise_matrix = (categories == categories.T).float().to(device)\n",
    "    distance_matrix = torch.cdist(re_pos_esm, re_pos_esm, p=2).to(device)  \n",
    "    positive_loss = pairwise_matrix * torch.pow(distance_matrix, 2)\n",
    "    negative_loss = (1 - pairwise_matrix) * torch.pow(torch.clamp(margin - distance_matrix, min=0.0), 2)\n",
    "    CL = torch.mean(positive_loss + negative_loss)\n",
    "\n",
    "    return BCE + CL, BCE, CL\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x):\n",
    "    BCE = nn.functional.cross_entropy(recon_x, x, label_smoothing=0.2)\n",
    "    return BCE\n",
    "\n",
    "model = Decoder_esm_to_seq_with_Contrastive(1280,4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3defa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (embed, labels, l_binary) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs, esm_x, label = model(embed[:,1:,:].float().to(device), l_binary)\n",
    "        labels_onehot = nn.functional.one_hot(labels.squeeze().to(torch.int64), num_classes=21).permute(0,2,1) # \n",
    "        # loss = loss_function(recon_x = outputs, x = labels_onehot.squeeze().to(device).float()) # only compute reconstruction loss\n",
    "        labels_onehot = nn.functional.one_hot(labels.squeeze().to(torch.int64), num_classes=21).permute(0,2,1) # \n",
    "        loss, BCE, CL = loss_function2(recon_x = outputs, \n",
    "                              x = labels_onehot.squeeze().to(device).float(), \n",
    "                              re_pos_esm=esm_x,\n",
    "                              labels=l_binary)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    diff_mask = torch.ne(outputs.argmax(dim=1).int().cpu(), labels_onehot.argmax(dim=1).int().cpu())\n",
    "    diff_count = torch.mean(torch.sum(diff_mask, dim=1).float())\n",
    "    max_count = torch.max(torch.sum(diff_mask, dim=1).float())\n",
    "    mse = nn.functional.mse_loss(outputs.argmax(dim=1).float().cpu(), labels_onehot.argmax(dim=1).float().cpu())\n",
    "    print(\"train reconstruction\",diff_count, 'max', max_count, 'mse', mse)\n",
    "    print(\"loss\",loss.cpu().item())\n",
    "    \n",
    "    if (epoch + 1) % 3 ==0:\n",
    "            model.eval()\n",
    "            valid_probs = []\n",
    "            valid_cls_labels = []\n",
    "            for i, (embed, labels, l_binary) in enumerate(valid_loader):\n",
    "                # Forward pass\n",
    "                outputs, esm_x, label = model(embed[:,1:,:].float().to(device), l_binary)\n",
    "                labels_onehot = nn.functional.one_hot(labels.squeeze().to(torch.int64), num_classes=21).permute(0,2,1) # \n",
    "                # loss = loss_function(recon_x = outputs, x = labels_onehot.squeeze().to(device).float())\n",
    "                labels_onehot = nn.functional.one_hot(labels.squeeze().to(torch.int64), num_classes=21).permute(0,2,1) # \n",
    "                loss, BCE, CL = loss_function2(recon_x = outputs, x = labels_onehot.squeeze().to(device).float(),\n",
    "                              re_pos_esm=esm_x,\n",
    "                              labels=l_binary)\n",
    "\n",
    "            print(\"                valid loss\",loss.cpu().item())\n",
    "            diff_mask = torch.ne(outputs.argmax(dim=1).int().cpu(), labels_onehot.argmax(dim=1).int().cpu())\n",
    "            diff_count = torch.mean(torch.sum(diff_mask, dim=1).float())\n",
    "            max_count = torch.max(torch.sum(diff_mask, dim=1).float())\n",
    "            mse = nn.functional.mse_loss(outputs.argmax(dim=1).float().cpu(), labels_onehot.argmax(dim=1).float().cpu())\n",
    "            print(\"      valid reconstruction\",diff_count, 'max', max_count, 'mse', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
